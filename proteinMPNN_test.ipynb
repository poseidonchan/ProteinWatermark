{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e99e6b-ca1c-4773-b1ee-f512f66e8c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union\n",
    "\n",
    "class AbstractWatermarkCode(ABC):\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def from_random(cls,\n",
    "                    rng: Union[np.random.Generator, list[np.random.Generator]],\n",
    "                    vocab_size: int):\n",
    "        pass\n",
    "\n",
    "class AbstractReweight(ABC):\n",
    "    watermark_code_type: type[AbstractWatermarkCode]\n",
    "\n",
    "    @abstractmethod\n",
    "    def reweight(self,\n",
    "                 code: AbstractWatermarkCode,\n",
    "                 p: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_la_score(self,\n",
    "                     code: AbstractWatermarkCode) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "def get_gumbel_variables(rng: np.random.Generator,\n",
    "                         vocab_size: int):\n",
    "    u = rng.random((vocab_size,))  # ~ Unifom(0, 1)\n",
    "    e = -np.log(u)  # ~ Exp(1)\n",
    "    g = -np.log(e)  # ~ Gumbel(0, 1)\n",
    "    return u, e, g\n",
    "\n",
    "\n",
    "class DeltaGumbel_WatermarkCode(AbstractWatermarkCode):\n",
    "    def __init__(self, g: np.ndarray):\n",
    "        self.g = g\n",
    "\n",
    "    @classmethod\n",
    "    def from_random(\n",
    "            cls,\n",
    "            rng: Union[np.random.Generator, list[np.random.Generator]],\n",
    "            vocab_size: int,\n",
    "    ):\n",
    "        if isinstance(rng, list):\n",
    "            batch_size = len(rng)\n",
    "            g = np.stack(\n",
    "                [get_gumbel_variables(rng[i], vocab_size)[2] for i in range(batch_size)]\n",
    "            )\n",
    "        else:\n",
    "            g = get_gumbel_variables(rng, vocab_size)[2]\n",
    "\n",
    "        return cls(g)\n",
    "\n",
    "\n",
    "class DeltaGumbel_Reweight(AbstractReweight):\n",
    "    watermark_code_type = DeltaGumbel_WatermarkCode\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"DeltaGumbel_Reweight()\"\n",
    "\n",
    "    def reweight(\n",
    "            self, code: AbstractWatermarkCode, p_logits: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        assert isinstance(code, DeltaGumbel_WatermarkCode)\n",
    "\n",
    "        index = np.argmax(p_logits + code.g, axis=-1)\n",
    "\n",
    "        mask = np.arange(p_logits.shape[-1]) == index[..., None]\n",
    "\n",
    "        modified_logits = np.where(\n",
    "            mask,\n",
    "            np.full_like(p_logits, 0),\n",
    "            np.full_like(p_logits, float(\"-inf\")),\n",
    "        )\n",
    "        return modified_logits\n",
    "\n",
    "    def get_la_score(self, code):\n",
    "        \"\"\"likelihood agnostic score\"\"\"\n",
    "        return np.array(np.log(2)) - np.exp(-code.g)\n",
    "\n",
    "class WatermarkDetector:\n",
    "    def __init__(\n",
    "            self,\n",
    "            private_key: any,\n",
    "            reweight: AbstractReweight,\n",
    "            context_code_length: int,\n",
    "            vocab_size: int = 20,\n",
    "            ignore_history: bool = False\n",
    "    ):\n",
    "        self.private_key = private_key\n",
    "        self.cc_length = context_code_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.reweight = reweight\n",
    "        self.ignore_history = ignore_history\n",
    "        self.cc_history = set()\n",
    "\n",
    "    def reset_history(self):\n",
    "        self.cc_history = set()\n",
    "\n",
    "    def get_rng_seed(self, context_code: any) -> any:\n",
    "        if not self.ignore_history:\n",
    "            self.cc_history.add(context_code)\n",
    "        import hashlib\n",
    "\n",
    "        m = hashlib.sha256()\n",
    "        m.update(context_code)\n",
    "        m.update(self.private_key)\n",
    "        full_hash = m.digest()\n",
    "        seed = int.from_bytes(full_hash, \"big\") % (2 ** 32 - 1)\n",
    "        return seed\n",
    "\n",
    "    def _get_codes(self, context):\n",
    "        batch_size = len(context)\n",
    "\n",
    "        context_codes = [\n",
    "            context[i][-self.cc_length:].tobytes() for i in range(batch_size)\n",
    "        ]\n",
    "        # print(context[0][-self.cc_length:])\n",
    "        mask, seeds = zip(\n",
    "            *[\n",
    "                (context_code in self.cc_history, self.get_rng_seed(context_code))\n",
    "                for context_code in context_codes\n",
    "            ]\n",
    "        )\n",
    "        return mask, seeds\n",
    "\n",
    "    def detect(self,\n",
    "               input_ids: np.ndarray):\n",
    "        \"\"\"\n",
    "        :param input_ids: sequences after tokenization\n",
    "        :return: scores, a higher score means a seq is likely to be watermarked\n",
    "        \"\"\"\n",
    "\n",
    "        scores = []\n",
    "        for i in range(input_ids.shape[1]):\n",
    "            score = self.get_la_score(input_ids[:, :i], input_ids[:, i], self.vocab_size)\n",
    "            \n",
    "            ti = score-np.log(2)\n",
    "            scores.append(ti)\n",
    "        assert np.all(ti<=0), ti\n",
    "        tis = np.array(scores)\n",
    "        uis = np.exp(tis)\n",
    "        Ubar=np.mean(uis)\n",
    "        print(\"Ubar\", Ubar)\n",
    "\n",
    "        if np.mean(uis)==0:\n",
    "            final_score=0\n",
    "            final_p_value=1\n",
    "            return final_score, final_p_value\n",
    "        avgS = lambda Ubar, lamb: Ubar*lamb+np.log(lamb/np.expm1(lamb))\n",
    "        import scipy.optimize\n",
    "        sol=scipy.optimize.minimize(lambda l:-avgS(Ubar, l), 0.5, bounds=[(0,10)])\n",
    "        final_score=-sol.fun*input_ids.shape[1]\n",
    "        final_p_value = np.exp(-final_score)\n",
    "        return final_score, final_p_value\n",
    "        \n",
    "        A = np.mean(tis)\n",
    "        final_score = (-1-A-np.log(-A))*input_ids.shape[1]\n",
    "        final_p_value = np.exp(-final_score)\n",
    "\n",
    "        print(\"optimal score is\", final_score)\n",
    "        print(\"optimal p-value is\", final_p_value)\n",
    "        return final_score, final_p_value\n",
    "\n",
    "    def get_la_score(\n",
    "            self,\n",
    "            input_ids: np.ndarray,\n",
    "            labels: np.ndarray,\n",
    "            vocab_size: int,\n",
    "    ) -> np.ndarray:\n",
    "        assert \"get_la_score\" in dir(\n",
    "            self.reweight\n",
    "        ), \"Reweight does not support likelihood agnostic detection\"\n",
    "        mask, seeds = self._get_codes(input_ids)\n",
    "        rng = [\n",
    "            np.random.default_rng(seed) for seed in seeds\n",
    "        ]\n",
    "        \n",
    "        mask = np.array(mask)\n",
    "        watermark_code = self.reweight.watermark_code_type.from_random(rng, vocab_size)\n",
    "        all_scores = self.reweight.get_la_score(watermark_code)\n",
    "        scores = all_scores[np.arange(all_scores.shape[0]), labels]\n",
    "        \n",
    "        scores = np.logical_not(mask) * scores\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea77fb20-6aa7-4556-8a91-2b69b35b0d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "amino_acids = 'ACDEFGHIKLMNPQRSTVWYX'  # List of amino acids\n",
    "# Create a dictionary mapping each amino acid to its index\n",
    "aa_to_index = {aa: idx for idx, aa in enumerate(amino_acids)}\n",
    "class AminoAcidTokenizer:\n",
    "    def __init__(self, aa_to_index):\n",
    "        self.aa_to_index = aa_to_index\n",
    "        self.index_to_aa = {idx: aa for aa, idx in aa_to_index.items()}\n",
    "        \n",
    "    def encode(self, sequence):\n",
    "        # Encode a sequence of amino acids to indices\n",
    "        return np.array([self.aa_to_index.get(aa, self.aa_to_index.get('X')) for aa in sequence]).reshape(1,-1)\n",
    "        \n",
    "    def decode(self, indices):\n",
    "        # Decode a list of indices back into an amino acid sequence\n",
    "        return ''.join(self.index_to_aa.get(idx, 'X') for idx in indices)\n",
    "tokenizer = AminoAcidTokenizer(aa_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fb033ce-3a08-4638-8169-6bc7d6ae9564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7ZEE design t=0.1\n",
    "seq_1 = \"MSLEEETATLDHPNVRIADPARVAEILAALRAGGADALRVVSDFDGTLSLVTKDGVPQPSLDDVLYNSPYISEEAKAKLDALDAEYTPIFNDPNLTVEQKLPYAKEYKTKKLEILTTENIKKSQIKEAVEKSGVKLREGAKRFFTLLEEHGVPLVIFSDGIGDIVEELIKSNNLLYPNIKIVANFFKYDENGNLVGFEGKLVTRFNKNATLEXXXXXXXXXXSHTHVILLGDSLSDINMTDGLPGITTELRIGFLNSDIEKNLEKFLATFDIVLVQDESLYVVNGILEEILG\"\n",
    "# 7ZEE design t=0.3\n",
    "seq_2 = \"LPVSAEKASLEHPHVRIADPARVAAILAALRAGGADALRVVSDFDGTLSLAKKNGVPQPSLNDVLKNSDVVSDEAKAKLKEIDEKYLPILNDPNLSKEEKLPYAKEYTTEKLEILKTENIKKSQIKEVVEKSGVKLREGAKRFFTLLEEHGVPLVIFSSGIGDIAEELIKSNNLLHSNITIVANFFKYDENGNLVGFEGKLVNKLNKNAKNLXXXXXXXXXXAHTHVILLGDSLSDIEMTEGLPGVTTELRIGFLNDSIEEKLEEFLARFDIVLVDDESLFVVNGILDDVLG\"\n",
    "# 7ZEE design t=0.5\n",
    "seq_3 = \"LPVSAEKASLEHPHVRIADPKRVADILEQLREGGSDRLAIVSDFDRTLSASFKDGVPQPSMDDVLKNSDVVSDEAKAEFAKLDAEYTPIFDDPNLTVAEKIPFAQKYYAEKLAILTKEEIKESQIAEMVRKSNVRLKEGAKRFFNLANEHKIPLYIFSAGIGDIKKELIRENGLYHDNIHLISNFFKFNEEGKLVGFEGALVTRFNKNMNNYXXXXXXXXXXNRTHVILIGDSLDDLDMHKGMEGITTLLSIGFLRSDIETNLKKFLDSFDIVLVQDESLYVVNGILDYITG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3f34aa2-7b34-49db-8d25-fc5b59465949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "original_1 = [[10, 15, 9, 3, 3, 3, 16, 0, 16, 9, 2, 6, 12, 11, 17, 14, 7, 0, 2, 12, 0, 14, 17, 0,\n",
    "             3, 7, 9, 0, 0, 9, 14, 0, 5, 5, 0, 2, 0, 9, 14, 17, 17, 15, 2, 4, 2, 5, 16, 9,\n",
    "             15, 9, 17, 16, 8, 2, 5, 17, 12, 13, 12, 15, 9, 2, 2, 17, 9, 19, 11, 15, 12, 19,\n",
    "             7, 15, 3, 3, 0, 8, 0, 8, 9, 2, 0, 9, 2, 0, 3, 19, 16, 12, 7, 4, 11, 2, 12, 11,\n",
    "             9, 16, 17, 3, 13, 8, 9, 12, 19, 0, 8, 3, 19, 8, 16, 8, 8, 9, 3, 7, 9, 16, 16,\n",
    "             3, 11, 7, 8, 8, 15, 13, 7, 8, 3, 0, 17, 3, 8, 15, 5, 17, 8, 9, 14, 3, 5, 0, 8,\n",
    "             14, 4, 4, 16, 9, 9, 3, 3, 6, 5, 17, 12, 9, 17, 7, 4, 15, 2, 5, 7, 5, 2, 7, 17,\n",
    "             3, 3, 9, 7, 8, 15, 11, 11, 9, 9, 19, 12, 11, 7, 8, 7, 17, 0, 11, 4, 4, 8, 19,\n",
    "             2, 3, 11, 5, 11, 9, 17, 5, 4, 3, 5, 8, 9, 17, 16, 14, 4, 11, 8, 11, 0, 16, 9,\n",
    "             3, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 15, 6, 16, 6, 17, 7, 9, 9, 5, 2, 15,\n",
    "             9, 15, 2, 7, 11, 10, 16, 2, 5, 9, 12, 5, 7, 16, 16, 3, 9, 14, 7, 5, 4, 9, 11,\n",
    "             15, 2, 7, 3, 8, 11, 9, 3, 8, 4, 9, 0, 16, 4, 2, 7, 17, 9, 17, 13, 2, 3, 15, 9,\n",
    "             19, 17, 17, 11, 5, 7, 9, 3, 3, 7, 9, 5]]\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "original_1 = np.array(original_1)\n",
    "print(np.allclose(tokenizer.encode(seq_1), original_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f30730e5-661a-487b-abd4-fd5110c977f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "original_2 = [[9, 12, 17, 15, 0, 3, 8, 0, 15, 9, 3, 6, 12, 6, 17, 14, 7, 0, 2, 12, 0, 14, 17, 0,\n",
    "            0, 7, 9, 0, 0, 9, 14, 0, 5, 5, 0, 2, 0, 9, 14, 17, 17, 15, 2, 4, 2, 5, 16, 9,\n",
    "            15, 9, 0, 8, 8, 11, 5, 17, 12, 13, 12, 15, 9, 11, 2, 17, 9, 8, 11, 15, 2, 17, 17,\n",
    "            15, 2, 3, 0, 8, 0, 8, 9, 8, 3, 7, 2, 3, 8, 19, 9, 12, 7, 9, 11, 2, 12, 11, 9, 15,\n",
    "            8, 3, 3, 8, 9, 12, 19, 0, 8, 3, 19, 16, 16, 3, 8, 9, 3, 7, 9, 8, 16, 3, 11, 7,\n",
    "            8, 8, 15, 13, 7, 8, 3, 17, 17, 3, 8, 15, 5, 17, 8, 9, 14, 3, 5, 0, 8, 14, 4, 4,\n",
    "            16, 9, 9, 3, 3, 6, 5, 17, 12, 9, 17, 7, 4, 15, 15, 5, 7, 5, 2, 7, 0, 3, 3, 9,\n",
    "            7, 8, 15, 11, 11, 9, 9, 6, 15, 11, 7, 16, 7, 17, 0, 11, 4, 4, 8, 19, 2, 3, 11,\n",
    "            5, 11, 9, 17, 5, 4, 3, 5, 8, 9, 17, 11, 8, 9, 11, 8, 11, 0, 8, 11, 9, 20, 20, 20,\n",
    "            20, 20, 20, 20, 20, 20, 20, 0, 6, 16, 6, 17, 7, 9, 9, 5, 2, 15, 9, 15, 2, 7, 3,\n",
    "            10, 16, 3, 5, 9, 12, 5, 17, 16, 16, 3, 9, 14, 7, 5, 4, 9, 11, 2, 15, 7, 3, 3,\n",
    "            8, 9, 3, 3, 4, 9, 0, 14, 4, 2, 7, 17, 9, 17, 2, 2, 3, 15, 9, 4, 17, 17, 11, 5,\n",
    "            7, 9, 2, 2, 17, 9, 5]]\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "original_2 = np.array(original_2)\n",
    "print(np.allclose(tokenizer.encode(seq_2), original_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c753967-c36a-40c1-aec2-92e9398bba6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "original_3 = [[9, 12, 17, 15, 0, 3, 8, 0, 15, 9, 3, 6, 12, 6, 17, 14, 7, 0, 2, 12, 8, 14, 17, 0,\n",
    "            2, 7, 9, 3, 13, 9, 14, 3, 5, 5, 15, 2, 14, 9, 0, 7, 17, 15, 2, 4, 2, 14, 16, 9,\n",
    "            15, 0, 15, 4, 8, 2, 5, 17, 12, 13, 12, 15, 10, 2, 2, 17, 9, 8, 11, 15, 2, 17, 17,\n",
    "            15, 2, 3, 0, 8, 0, 3, 4, 0, 8, 9, 2, 0, 3, 19, 16, 12, 7, 4, 2, 2, 12, 11, 9, 16,\n",
    "            17, 0, 3, 8, 7, 12, 4, 0, 13, 8, 19, 19, 0, 3, 8, 9, 0, 7, 9, 16, 8, 3, 3, 7, 8, 3,\n",
    "            15, 13, 7, 0, 3, 10, 17, 14, 8, 15, 11, 17, 14, 9, 8, 3, 5, 0, 8, 14, 4, 4, 11, 9, 0,\n",
    "            11, 3, 6, 8, 7, 12, 9, 19, 7, 4, 15, 0, 5, 7, 5, 2, 7, 8, 8, 3, 9, 7, 14, 3, 11, 5, 9,\n",
    "            19, 6, 2, 11, 7, 6, 9, 7, 15, 11, 4, 4, 8, 4, 11, 3, 3, 5, 8, 9, 17, 5, 4, 3, 5, 0, 9,\n",
    "            17, 16, 14, 4, 11, 8, 11, 10, 11, 11, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 11,\n",
    "            14, 16, 6, 17, 7, 9, 7, 5, 2, 15, 9, 2, 2, 9, 2, 10, 6, 8, 5, 10, 3, 5, 7, 16, 16, 9,\n",
    "            9, 15, 7, 5, 4, 9, 14, 15, 2, 7, 3, 16, 11, 9, 8, 8, 4, 9, 2, 15, 4, 2, 7, 17, 9, 17,\n",
    "            13, 2, 3, 15, 9, 19, 17, 17, 11, 5, 7, 9, 2, 19, 7, 16, 5]]\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "original_3 = np.array(original_3)\n",
    "print(np.allclose(tokenizer.encode(seq_3), original_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0bd5065-b6f1-4986-8e91-a8dd2f868c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ubar 0.5067641594786917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.08016515144748818, 0.9229639049741365)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector = WatermarkDetector(b\"private key\",\n",
    "                             DeltaGumbel_Reweight(),\n",
    "                             context_code_length=5,\n",
    "                             vocab_size=30)\n",
    "detector.detect(tokenizer.encode(seq_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f63a0da-bab4-4b11-82bb-8602b3e2a7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ubar 0.5747777000532985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.86334274150776, 5.204807529922428e-05)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector = WatermarkDetector(b\"private key\",\n",
    "                             DeltaGumbel_Reweight(),\n",
    "                             context_code_length=5,\n",
    "                             vocab_size=30)\n",
    "detector.detect(tokenizer.encode(seq_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66c1568a-c77b-4962-9dae-00977c6eafa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ubar 0.6532691784403173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42.391178039335486, 3.8881704573624797e-19)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector = WatermarkDetector(b\"private key\",\n",
    "                             DeltaGumbel_Reweight(),\n",
    "                             context_code_length=5,\n",
    "                             vocab_size=30)\n",
    "detector.detect(tokenizer.encode(seq_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aaac2f-071b-4ddc-8c73-15b4948f452f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e32ae4-c166-4897-b708-0be32548c874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
